# Week 04 — Visual Music: Music Video Exercise

**Course:** Vibecoding GLSL Shaders | CalArts Program in Experimental Animation

**Format:** Work alone or in pairs

**Duration:** 90–120 minutes in-class production + 15 minutes screening/discussion

**Output:** 60–120 second visual-music study

## Where We Are

You've explored the exercises and learned how to use Claude/ChatGPT/DeepSeek to change example code with a prompt. Now we put those skills to work in a constrained creative exercise: making a short visual-music piece driven by chance.

## Overview

Create a short visual-music piece by drawing constraints from chance: a real song (from the top 10 in your birth year) and one of thirteen shader-based techniques. Each technique maps to a working section in the codebase — start from the existing shader and modify it with LLM assistance. The goal is to treat the music as a found object and technique as your primary constraint.

*Note on complexity:* some draws are more accessible than others. Warps and Scribble involve tweaking parameters on image-reactive shaders; Particles and Reaction Diffusion involve multi-pass GPU simulations. If your draw requires concepts we haven't covered yet, you may swap once or focus on modifying the existing shader's visual parameters rather than rewriting its architecture.

## The Two Hats System

We will work with two "hats":

**Hat A: Music** — slips with a real track + artist written small on the back, from the year of your birth.

**Hat B: Techniques** — slips with one of the thirteen shader/visual-music techniques.

## Techniques Hat

- **Kaleidoscope** — Radial, kaleidoscopic structures that evolve like computational cinema.
- **Geometries** — Shader-Park-style SDF forms: morph, orbit, and slice in sync with musical sections.
- **Op Art** — High-contrast stripes, waves, and grids that shimmer and pulse with rhythm.
- **Glyphs** — ASCII fields or invented alphabets that appear, recombine, and morph like a graphic score.
- **Stipples** — Fields of dots whose density, clustering, and motion map to sound texture.
- **Particles** — GPU particles driven by audio as forces: bursts, gravity wells, sprays, and flows.
- **Characters** — Procedural creatures whose bodies or environments deform with the sound.
- **Warps** — Continuous bending, smearing, and swirling of imagery like a liquid screen.
- **Tiles** — Voronoi, hexgrid, and tiling patterns that subdivide and recolor in sync.
- **Landscapes** — Raymarched terrain with strobes, lightning, and atmospheric flashes.
- **Displacements** — Vertex displacement with textures: surfaces that ripple, crumple, or breathe.
- **Scribble** — Image-reactive drawing: scribble circles or hatching lines driven by luminance.
- **Reaction Diffusion** — Gray-Scott patterns on curved surfaces, mutating over time.

## In-Class Protocol

### Phase 1: Draw (~5 minutes)

Working in pairs:

1. Draw one slip from **Hat A**.
   - You see the false title.
   - Flip to see the real track + artist you must use.
2. Draw one slip from **Hat B**.
   - This is your primary visual-music technique.

You now have:
- A **real song** (audio to work with).
- A **false title** (conceptual frame / paratext).
- A **technique** (visual grammar / constraint).

### Phase 2: Concept + Strategy (~15 minutes)

On paper or in a shared doc, quickly sketch:

1. **What audio features will you map to visuals?**
   - Rhythm / meter
   - Dynamics (quiet vs loud, build-ups, drops)
   - Spectral bands (bass, mids, highs)
   - Specific events (hits, glitches, vocal entries)

2. **Given your technique, what will be your primary mapping?**
   - E.g., for Particles: bass = gravity, snare = bursts, hi-hats = sparkle noise.

3. **Working in pairs?** How will you divide labor?
   - One drives code/patch, one directs timing and composition — then swap if possible.

You may illustrate the lyrics if you can find a fun way to do it, but this isn't required or denied.

### Phase 3: Production (~90–120 minutes)

Build your visual-music study using the shader-playground codebase and your LLM of choice. Record a 60–120 second screen capture of the result synced to the music.

### Phase 4: Screening + Discussion (~15 minutes)

We watch everyone's pieces together and discuss: What worked? What surprised you? How did the constraint shape the outcome?
